# app.py
# app.py

import streamlit as st
import requests
import os
import concurrent.futures
import time
from dotenv import load_dotenv
from models_config import models

load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# Check if API key is available
if not OPENROUTER_API_KEY:
    st.error("âŒ OPENROUTER_API_KEY is not set in your .env file. Please add it and restart the app.")
    st.stop()

headers = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "HTTP-Referer": "https://yourdomain.com",
    "X-Title": "LLM Compare App"
}

def query_model(prompt, model_name):
    url = "https://openrouter.ai/api/v1/chat/completions"
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }

    try:
        # Add timeout to prevent hanging
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            return f"âŒ Error: {response.status_code} - {response.text}"
    except requests.exceptions.Timeout:
        return f"âŒ Timeout: Request to {model_name} timed out after 60 seconds"
    except requests.exceptions.RequestException as e:
        return f"âŒ Request Error: {str(e)}"
    except Exception as e:
        return f"âŒ Unexpected Error: {str(e)}"

# --- Streamlit UI ---
st.set_page_config(page_title="LLM æ¯”è¼ƒãƒ“ãƒ¥ãƒ¼", layout="wide")
st.title("ğŸ¤– Momongaã‚¢ãƒªãƒ¼ãƒŠ")

# Initialize session state for conversation history
if 'conversation' not in st.session_state:
    st.session_state.conversation = []

# ãƒ¢ãƒ‡ãƒ«é¸æŠ
st.subheader("ğŸ” æ¯”è¼ƒã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã‚“ã§ãã ã•ã„")
selected_model_names = st.multiselect(
    "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«", options=[m["name"] for m in models], default=[models[0]["name"]]
)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›
prompt = st.text_area("ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=150)

if st.button("é€ä¿¡ï¼") and prompt and selected_model_names:
    # Add the prompt to the conversation history
    st.session_state.conversation.append({"role": "user", "content": prompt})
    
    # é¸ã°ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—
    selected_models = [m for m in models if m["name"] in selected_model_names]

    # ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã®é–¢æ•°
    def process_model(model):
        start_time = time.time()
        result = query_model(prompt, model["model"])
        elapsed_time = time.time() - start_time
        return model["name"], result, elapsed_time
    
    # ä¸¦åˆ—å‡¦ç†ã®å®Ÿè¡Œ
    with st.spinner("å¿œç­”ã‚’å–å¾—ä¸­..."):
        with concurrent.futures.ThreadPoolExecutor() as executor:
            # ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ä¸¦åˆ—ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
            future_to_model = {executor.submit(process_model, model): model for model in selected_models}
            
            # çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
            results = {}
            elapsed_times = {}
            
            # å„ãƒ¢ãƒ‡ãƒ«ã®åˆ—ã‚’ä½œæˆ
            cols = st.columns(len(selected_models))
            
            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—ã—ã€å³åº§ã«è¡¨ç¤º
            for future in concurrent.futures.as_completed(future_to_model):
                model_name, response, elapsed = future.result()
                results[model_name] = response
                elapsed_times[model_name] = elapsed
                
                # Add the response to the conversation history
                st.session_state.conversation.append({"role": "assistant", "model": model_name, "content": response})
                
                # çµæœã®è¡¨ç¤º
                index = selected_model_names.index(model_name)
                with cols[index]:
                    st.markdown(f"### {model_name}")
                    st.markdown(f"*å¿œç­”æ™‚é–“: {elapsed_times[model_name]:.2f}ç§’*")
                    st.write(response)

# Display the conversation history
st.subheader("ğŸ—¨ï¸ ä¼šè©±å±¥æ­´")
for entry in st.session_state.conversation:
    if entry["role"] == "user":
        st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼:** {entry['content']}")
    else:
        st.markdown(f"**{entry['model']} ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:** {entry['content']}")
